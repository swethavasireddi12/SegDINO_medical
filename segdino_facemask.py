# -*- coding: utf-8 -*-
"""SegDINO_FaceMask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JItK3kLW9DOMpBuKsOA62JYGILs0hVYE
"""

!pip install -q torch torchvision timm tqdm pillow kaggle

# Upload your kaggle.json (from your Kaggle account)
from google.colab import files
files.upload()  # choose kaggle.json when prompted

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d perke986/face-mask-segmentation-dataset
!unzip -q face-mask-segmentation-dataset.zip -d face-mask-segmentation-dataset
!ls face-mask-segmentation-dataset/images | head
!ls face-mask-segmentation-dataset/masks | head

import os, torch, timm
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

# ---------------- Dataset ----------------
class FaceMaskSegDataset(Dataset):
    def __init__(self, root_dir, img_size=256):
        self.root_dir = root_dir
        self.images_dir = os.path.join(root_dir, "images")
        self.masks_dir = os.path.join(root_dir, "masks")
        self.img_names = sorted(os.listdir(self.images_dir))
        self.tf_img = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])
        self.tf_mask = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])

    def __len__(self): return len(self.img_names)

    def __getitem__(self, idx):
        name = self.img_names[idx]
        img_path  = os.path.join(self.images_dir, name)
        mask_path = os.path.join(self.masks_dir, os.path.splitext(name)[0] + ".png")
        img  = self.tf_img(Image.open(img_path).convert("RGB"))
        mask = self.tf_mask(Image.open(mask_path).convert("L"))
        mask = (mask > 0.5).long().squeeze(0)
        return img, mask

# ---------------- ViT Backbone ----------------
import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

class ViTExtractor(nn.Module):
    """
    Extracts intermediate features from a pretrained Vision Transformer (DINO or ViT backbone).
    We register forward hooks on the selected transformer blocks.
    """
    def __init__(self, model_name="vit_base_patch16_224", selected_layers=[2,5,8,11]):
        super().__init__()
        self.model = timm.create_model(model_name, pretrained=True)
        self.selected_layers = selected_layers
        self.patch_size = self.model.patch_embed.patch_size[0]
        self.embed_dim = self.model.embed_dim
        self._features = {}

        # Register hooks to capture intermediate outputs
        for i, blk in enumerate(self.model.blocks):
            if i in selected_layers:
                blk.register_forward_hook(self._hook(i))

    def _hook(self, idx):
        def fn(_, __, output):
            self._features[idx] = output
        return fn

    def forward(self, x):
        self._features = {}
        _ = self.model.forward_features(x)
        feats = [self._features[i] for i in sorted(self._features.keys())]
        out = []
        for f in feats:
            # Remove CLS token if present
            if f.shape[1] > (x.shape[2] // self.patch_size) ** 2:
                f = f[:, 1:, :]
            out.append(f)
        return out


# ---------------- SegDINO Decoder ----------------
class SegDINO(nn.Module):
    """
    Lightweight segmentation decoder for DINO/ViT features.
    """
    def __init__(self, backbone, n_classes=2, c_out=64):
        super().__init__()
        self.backbone = backbone
        self.c_out = c_out
        self.n_classes = n_classes
        self.align = None
        self.head = None

    def _init_layers(self, n_levels, embed_dim):
        # simple linear alignment and lightweight segmentation head
        self.align = nn.ModuleList([
            nn.Sequential(nn.Linear(embed_dim, self.c_out), nn.GELU())
            for _ in range(n_levels)
        ])
        self.head = nn.Sequential(
            nn.Conv2d(n_levels * self.c_out, n_levels * self.c_out, 1),
            nn.GELU(),
            nn.Conv2d(n_levels * self.c_out, self.n_classes, 1)
        )

    def forward(self, x):
        feats = self.backbone(x)
        if self.align is None:
            self._init_layers(len(feats), self.backbone.embed_dim)
            # âœ… Ensure decoder layers move to same device as input
            self.align = self.align.to(x.device)
            self.head = self.head.to(x.device)

        maps = []
        for i, f in enumerate(feats):
            B, N, C = f.shape
            H = W = int(N ** 0.5)
            f = self.align[i](f)
            f = f.permute(0, 2, 1).reshape(B, self.c_out, H, W)
            maps.append(f)

        # Upsample and concatenate multi-scale features
        maxH = max(m.shape[-2] for m in maps)
        maxW = max(m.shape[-1] for m in maps)
        ups = [F.interpolate(m, (maxH, maxW), mode="bilinear", align_corners=False) for m in maps]
        fused = torch.cat(ups, 1)
        out = self.head(fused)
        return F.interpolate(out, size=(x.shape[-2], x.shape[-1]), mode="bilinear", align_corners=False)

def dice_score(pred, mask):
    pred=(pred>0.5).float(); mask=mask.float()
    inter=(pred*mask).sum(); union=pred.sum()+mask.sum()
    return (2*inter+1e-6)/(union+1e-6)

def train_one_epoch(model, loader, opt, dev):
    model.train(); ce=nn.CrossEntropyLoss(); total=0
    for imgs,masks in tqdm(loader,desc="Training"):
        imgs,masks=imgs.to(dev),masks.to(dev)
        opt.zero_grad()
        out=model(imgs)
        loss=ce(out,masks)
        loss.backward(); opt.step()
        total+=loss.item()*imgs.size(0)
    return total/len(loader.dataset)

@torch.no_grad()
def evaluate(model, loader, dev):
    model.eval(); dices=[]
    for imgs,masks in tqdm(loader,desc="Evaluating"):
        imgs,masks=imgs.to(dev),masks.to(dev)
        out=model(imgs)
        probs=torch.softmax(out,1)[:,1]
        dices.append(dice_score(probs,masks).item())
    return np.mean(dices)

data_root = "face-mask-segmentation-dataset"
img_size = 224    # âœ… changed
batch_size = 4
epochs = 25
lr = 1e-4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

dataset = FaceMaskSegDataset(data_root, img_size)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(dataset, batch_size=1, shuffle=False)

backbone = ViTExtractor("vit_base_patch16_224", [2,5,8,11])
model = SegDINO(backbone, n_classes=2).to(device)

for p in backbone.parameters():
    p.requires_grad = False

# ðŸ”§ Dummy input to initialize decoder
dummy = torch.randn(1, 3, img_size, img_size).to(device)
_ = model(dummy)

opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr)

for e in range(epochs):
    loss = train_one_epoch(model, loader, opt, device)
    dice = evaluate(model, val_loader, device)
    print(f"Epoch {e+1}/{epochs} | Loss={loss:.4f} | Dice={dice:.4f}")

torch.save(model.state_dict(), "segdino_facemask.pth")
print("âœ… Training complete! Model saved as segdino_facemask.pth")

import matplotlib.pyplot as plt
model.eval()

# Pick some random samples
for idx in [0, 50, 120, 200]:
    img, mask = dataset[idx]
    with torch.no_grad():
        pred = torch.softmax(model(img.unsqueeze(0).to(device)), 1)[0, 1].cpu()

    plt.figure(figsize=(10, 4))
    plt.subplot(1, 3, 1)
    plt.imshow(img.permute(1, 2, 0))
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(mask, cmap="gray")
    plt.title("Ground Truth Mask")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(pred > 0.5, cmap="gray")
    plt.title("Predicted Mask")
    plt.axis("off")

    plt.show()

train_losses, val_dices = [], []
best_dice = 0

for e in range(epochs):
    loss = train_one_epoch(model, loader, opt, device)
    dice = evaluate(model, val_loader, device)

    train_losses.append(loss)
    val_dices.append(dice)

    print(f"Epoch {e+1}/{epochs} | Loss={loss:.4f} | Dice={dice:.4f}")

    if dice > best_dice:
        best_dice = dice
        torch.save(model.state_dict(), "best_segdino_facemask.pth")
        print(f"âœ… Best model saved (Dice={dice:.4f})")

print("ðŸŽ‰ Training finished successfully!")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(range(1, len(train_losses)+1), train_losses, 'r-o', label='Training Loss')
plt.plot(range(1, len(val_dices)+1), val_dices, 'b-o', label='Validation Dice')
plt.xlabel("Epochs")
plt.ylabel("Value")
plt.title("Training Progress of SegDINO Model")
plt.legend()
plt.grid(True)
plt.show()



from sklearn.metrics import confusion_matrix

@torch.no_grad()
def compute_metrics(model, loader, device):
    model.eval()
    ious, precs, recs, f1s = [], [], [], []
    for imgs, masks in loader:
        imgs, masks = imgs.to(device), masks.to(device)
        out = model(imgs)
        probs = torch.softmax(out, 1)[:, 1]
        preds = (probs > 0.5).float().cpu().numpy().flatten()
        gt = masks.cpu().numpy().flatten()
        tn, fp, fn, tp = confusion_matrix(gt, preds, labels=[0,1]).ravel()
        iou = tp / (tp + fp + fn + 1e-6)
        prec = tp / (tp + fp + 1e-6)
        rec = tp / (tp + fn + 1e-6)
        f1 = 2 * prec * rec / (prec + rec + 1e-6)
        ious.append(iou); precs.append(prec); recs.append(rec); f1s.append(f1)
    return {
        "IoU": np.mean(ious),
        "Precision": np.mean(precs),
        "Recall": np.mean(recs),
        "F1": np.mean(f1s)
    }

metrics = compute_metrics(model, val_loader, device)
print("ðŸ“Š SegDINO Model Evaluation Metrics:")
for k,v in metrics.items():
    print(f"{k}: {v:.4f}")